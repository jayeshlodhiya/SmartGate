package com.payphi.visitorsregister.utils;

import android.os.AsyncTask;
import android.support.v7.app.AppCompatActivity;
import android.text.TextUtils;

import com.microsoft.projectoxford.face.FaceServiceClient;
import com.microsoft.projectoxford.face.FaceServiceRestClient;
import com.microsoft.projectoxford.face.contract.Accessory;
import com.microsoft.projectoxford.face.contract.Emotion;
import com.microsoft.projectoxford.face.contract.Face;
import com.microsoft.projectoxford.face.contract.FacialHair;
import com.microsoft.projectoxford.face.contract.Hair;
import com.microsoft.projectoxford.face.contract.HeadPose;
import com.microsoft.projectoxford.face.contract.Makeup;
import com.payphi.visitorsregister.RegisterFragment;

import java.io.ByteArrayInputStream;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

/**
 * Created by swapnil.g on 6/18/2018.
 */
public class FragDetectionTaskActivity extends AppCompatActivity {
    public static class FragDetectionTask extends AsyncTask<InputStream, String, Face[]> {
        private boolean mSucceed = true;

        private String getHair(Hair hair) {
            if (hair.hairColor.length == 0) {
                if (hair.invisible)
                    return "Invisible";
                else
                    return "Bald";
            } else {
                int maxConfidenceIndex = 0;
                double maxConfidence = 0.0;

                for (int i = 0; i < hair.hairColor.length; ++i) {
                    if (hair.hairColor[i].confidence > maxConfidence) {
                        maxConfidence = hair.hairColor[i].confidence;
                        maxConfidenceIndex = i;
                    }
                }

                return hair.hairColor[maxConfidenceIndex].color.toString();
            }
        }

        private String getMakeup(Makeup makeup) {
            return (makeup.eyeMakeup || makeup.lipMakeup) ? "Yes" : "No";
        }

        private String getAccessories(Accessory[] accessories) {
            if (accessories.length == 0) {
                return "NoAccessories";
            } else {
                String[] accessoriesList = new String[accessories.length];
                for (int i = 0; i < accessories.length; ++i) {
                    accessoriesList[i] = accessories[i].type.toString();
                }

                return TextUtils.join(",", accessoriesList);
            }
        }

        private String getFacialHair(FacialHair facialHair) {
            return (facialHair.moustache + facialHair.beard + facialHair.sideburns > 0) ? "Yes" : "No";
        }

        private String getEmotion(Emotion emotion) {
            String emotionType = "";
            double emotionValue = 0.0;
            if (emotion.anger > emotionValue) {
                emotionValue = emotion.anger;
                emotionType = "Anger";
            }
            if (emotion.contempt > emotionValue) {
                emotionValue = emotion.contempt;
                emotionType = "Contempt";
            }
            if (emotion.disgust > emotionValue) {
                emotionValue = emotion.disgust;
                emotionType = "Disgust";
            }
            if (emotion.fear > emotionValue) {
                emotionValue = emotion.fear;
                emotionType = "Fear";
            }
            if (emotion.happiness > emotionValue) {
                emotionValue = emotion.happiness;
                emotionType = "Happiness";
            }
            if (emotion.neutral > emotionValue) {
                emotionValue = emotion.neutral;
                emotionType = "Neutral";
            }
            if (emotion.sadness > emotionValue) {
                emotionValue = emotion.sadness;
                emotionType = "Sadness";
            }
            if (emotion.surprise > emotionValue) {
                emotionValue = emotion.surprise;
                emotionType = "Surprise";
            }
            return String.format("%s: %f", emotionType, emotionValue);
        }

        private String getHeadPose(HeadPose headPose) {
            return String.format("Pitch: %s, Roll: %s, Yaw: %s", headPose.pitch, headPose.roll, headPose.yaw);
        }

        @Override
        protected Face[] doInBackground(InputStream... params) {
            // Get an instance of face service client to detect faces in image.
            // FaceServiceClient faceServiceClient = SampleApp.getFaceServiceClient();
            String end_point = "https://westeurope.api.cognitive.microsoft.com/face/v1.0";
            String subscription_key = "8475695dd50d424cbcf6bf71a4b12f92";
            FaceServiceClient faceServiceClient = new FaceServiceRestClient(end_point, subscription_key);
            try {
                publishProgress("Detecting...");

                // Start detection.
                return faceServiceClient.detect(
                        params[0],  /* Input stream of image to detect */
                        true,       /* Whether to return face ID */
                        true,       /* Whether to return face landmarks */
                        /* Which face attributes to analyze, currently we support:
                           age,gender,headPose,smile,facialHair */
                        new FaceServiceClient.FaceAttributeType[]{
                                FaceServiceClient.FaceAttributeType.Age,
                                FaceServiceClient.FaceAttributeType.Gender,
                                FaceServiceClient.FaceAttributeType.Smile,
                                FaceServiceClient.FaceAttributeType.Glasses,
                                FaceServiceClient.FaceAttributeType.FacialHair,
                                FaceServiceClient.FaceAttributeType.Emotion,
                                FaceServiceClient.FaceAttributeType.HeadPose,
                                FaceServiceClient.FaceAttributeType.Accessories,
                                FaceServiceClient.FaceAttributeType.Blur,
                                FaceServiceClient.FaceAttributeType.Exposure,
                                FaceServiceClient.FaceAttributeType.Hair,
                                FaceServiceClient.FaceAttributeType.Makeup,
                                FaceServiceClient.FaceAttributeType.Noise,
                                FaceServiceClient.FaceAttributeType.Occlusion
                        });
            } catch (Exception e) {
                mSucceed = false;
                publishProgress(e.getMessage());
                //addLog(e.getMessage());
                return null;
            }
        }

        @Override
        protected void onPreExecute() {
            //   mProgressDialog.show();
            //  addLog("Request: Detecting in image " + mImageUri);
        }

        @Override
        protected void onProgressUpdate(String... progress) {
            // mProgressDialog.setMessage(progress[0]);
            //setInfo(progress[0]);
        }

        @Override
        protected void onPostExecute(Face[] result) {
            List<Face> faces = new ArrayList<>();
            int position = 0;
            try {
                faces = Arrays.asList(result);
                if (mSucceed) {
                    //addLog("Response: Success. Detected " + (result == null ? 0 : result.length) + " face(s) in " + mImageUri);
                    String face_description = String.format("Age: %s - Gender: %s-Hair: %s - FacialHair: %s-Makeup: %s  %s-ForeheadOccluded: %s  Blur: %s-EyeOccluded: %s  %s-" + "MouthOccluded: %s  Noise: %s-GlassesType: %s-HeadPose: %s-Accessories: %s",
                            faces.get(position).faceAttributes.age,
                            faces.get(position).faceAttributes.gender,
                            getHair(faces.get(position).faceAttributes.hair),
                            getFacialHair(faces.get(position).faceAttributes.facialHair),
                            getMakeup((faces.get(position)).faceAttributes.makeup),
                            getEmotion(faces.get(position).faceAttributes.emotion),
                            faces.get(position).faceAttributes.occlusion.foreheadOccluded,
                            faces.get(position).faceAttributes.blur.blurLevel,
                            faces.get(position).faceAttributes.occlusion.eyeOccluded,
                            faces.get(position).faceAttributes.exposure.exposureLevel,
                            faces.get(position).faceAttributes.occlusion.mouthOccluded,
                            faces.get(position).faceAttributes.noise.noiseLevel,
                            faces.get(position).faceAttributes.glasses,
                            getHeadPose(faces.get(position).faceAttributes.headPose),
                            getAccessories(faces.get(position).faceAttributes.accessories)

                    );
                    System.out.println("face_description in fragment=" + face_description);
                    //String facedesc =
                    RegisterFragment.getInstance().setFacialData(face_description);

                    // Show the result on screen when detection is done.
                    //   setUiAfterDetection(result, mSucceed);


                }
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    }

    public static void getFaceData(ByteArrayInputStream inputStream) {
        new FragDetectionTask().execute(inputStream);
    }
}